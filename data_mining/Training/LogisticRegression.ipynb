{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70380688",
   "metadata": {},
   "source": [
    "# **Regresion Logistica**\n",
    "\n",
    "La regresión logistica es un método estadístico y matemático que se utiliza para encontrar una clasificacion entre dos o más variables. Su objetico es predecir la probabilidad de que algo pertenezca a una clase (por ejemplo, sí/no, positivo/negativo, aprobado/reprobado…).\n",
    "\n",
    "¿Cómo funciona entonces? Al igual que un modelo de Regresión Lineal, un modelo de Regresión Logística calcula una suma ponderada de las características de entrada (más un término de error o bias), pero en lugar de arrojar el resultado directamente —como hace el modelo de Regresión Lineal—, arroja la logística de dicho resultado. Se trata de aprendizaje supervisado, porque se aprende a partir de un conjunto de ejemplos ya clasificados.\n",
    "\n",
    "Tenemos la suma ponderada siguiente:\n",
    "\n",
    "$$\n",
    "\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n = \\theta^T \\mathbf{x}\n",
    "$$\n",
    "\n",
    "Probabilidad estimada del modelo de Regresión Logística\n",
    "\n",
    "$$\n",
    "\\hat{p} = h_{\\theta}(\\mathbf{x}) = \\sigma(\\theta^T \\mathbf{x})\n",
    "$$\n",
    "\n",
    "La función logística —denotada como $\\sigma(\\cdot)$— es una función sigmoide (es decir, en forma de S) que devuelve un número entre 0 y 1. Se define como se muestra:\n",
    "\n",
    "$$\n",
    "\\sigma(t) = \\frac{1}{1 + \\exp(-t)}\n",
    "$$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/Sigmoide.png\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez que el modelo de Regresión Logística ha estimado la probabilidad $\\hat{p} = h_{\\theta}(\\mathbf{x})$ de que una instancia $\\mathbf{x}$ pertenezca a la clase positiva, puede realizar su predicción $\\hat{y}$ fácilmente:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\begin{cases} 0 & \\text{si } \\hat{p} < 0.5 \\\\ 1 & \\text{si } \\hat{p} \\geq 0.5 \\end{cases}\n",
    "$$\n",
    "\n",
    "Nótese que $\\sigma(t) < 0.5$ cuando $t < 0$, y $\\sigma(t) \\geq 0.5$ cuando $t \\geq 0$, por lo que un modelo de Regresión Logística predice $1$ si $\\mathbf{x}^T \\mathbf{\\theta}$ es positivo, y $0$ si es negativo. \n",
    "\n",
    "### Frontera de Decision\n",
    "\n",
    "La expresion de la funcion sigmoide nos abre a expresiones que sean lineales y no lineales resolviendo esto $\\theta^T \\mathbf{x} = 0$ pero teniendo los valores de $\\theta$, como los ejemplos de las siguientes imagenes:\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; gap: 40px;\">\n",
    "  <img src=\"img/LogitLineal.png\" alt=\"Izquierda\" width=\"400\">\n",
    "  <img src=\"img/LogitNoLineal.png\" alt=\"Derecha\" width=\"425\">\n",
    "</div>\n",
    "\n",
    "### Funcion de Coste\n",
    "\n",
    "La función de costo (también llamada función de pérdida) es una de las partes más importantes de la regresión logística, porque mide qué tan bien o mal está funcionando el modelo.\n",
    "\n",
    "El Coste asociado a cada ejemplo se define como:\n",
    "\n",
    "$$\n",
    "\\text{Coste}(h_\\theta(x), y) =\n",
    "\\begin{cases}\n",
    "-\\log\\big(h_\\theta(x)\\big), & \\text{si } y = 1,\\\\[8pt]\n",
    "-\\log\\big(1 - h_\\theta(x)\\big), & \\text{si } y = 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Es decir \n",
    "$$\n",
    "\\text{Coste}(h_\\theta(x), y) = -y\\log\\big(h_\\theta(x)\\big) -(1-y)\\log\\big(1 - h_\\theta(x)\\big)\n",
    "$$\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; gap: 40px;\">\n",
    "  <img src=\"img/CosteLogit1.png\" alt=\"Izquierda\" width=\"500\">\n",
    "  <img src=\"img/CosteLogit2.png\" alt=\"Derecha\" width=\"485\">\n",
    "</div>\n",
    "\n",
    "Coste total de todos los datos de entrenamiento del modelo es:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\Big[ y^{(i)} \\log\\big(h_\\theta(x^{(i)})\\big) + (1 - y^{(i)}) \\log\\big(1 - h_\\theta(x^{(i)})\\big) \\Big]\n",
    "$$\n",
    "\n",
    "#### Minimizacion del costo\n",
    "\n",
    "Hallaremos la **gradiente** $\\nabla_\\theta J(\\theta)$, pero recordemos lo siguiente $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$, entonces para un solo ejemplo:\n",
    "\n",
    "$$\n",
    "\\ell^{(i)}(\\theta) = -\\Big[ y^{(i)}\\log(\\sigma(z^{(i)})) + (1-y^{(i)})\\log(1-\\sigma(z^{(i)}))\\Big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\ell^{(i)}}{dz^{(i)}} \n",
    "= -\\left[ y^{(i)}\\frac{1}{\\sigma(z^{(i)})}\\sigma'(z^{(i)}) + (1-y^{(i)})\\frac{1}{1-\\sigma(z^{(i)})}(-\\sigma'(z^{(i)})) \\right].\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\ell^{(i)}}{dz^{(i)}} \n",
    "= -\\sigma'(z^{(i)})\\left[\\frac{y^{(i)}}{\\sigma(z^{(i)})} - \\frac{1-y^{(i)}}{1-\\sigma(z^{(i)})}\\right].\n",
    "$$\n",
    "\n",
    "Sustituyendo $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$ y simplificando:\n",
    "\n",
    "$$\n",
    "\\frac{d\\ell^{(i)}}{dz^{(i)}} = \\sigma(z^{(i)}) - y^{(i)} = h_\\theta(x^{(i)}) - y^{(i)}.\n",
    "$$\n",
    "\n",
    "Aplicamos la regla de la cadena respecto a $\\theta$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell^{(i)}}{\\partial \\theta} \n",
    "= \\frac{d\\ell^{(i)}}{dz^{(i)}} \\cdot \\frac{\\partial z^{(i)}}{\\partial \\theta}\n",
    "= \\big(h_\\theta(x^{(i)}) - y^{(i)}\\big) x^{(i)}.\n",
    "$$\n",
    "\n",
    "### Gradiente del Costo\n",
    "\n",
    "Asi que la gradiente sobre el conjunto m es:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)\n",
    "= \\frac{1}{m}\\sum_{i=1}^m \\frac{\\partial \\ell^{(i)}}{\\partial \\theta}\n",
    "= \\frac{1}{m}\\sum_{i=1}^m \\big(h_\\theta(x^{(i)}) - y^{(i)}\\big)\\, x^{(i)}.\n",
    "$$\n",
    "\n",
    "De forma vectorizada tenemos:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{1}{m} X^T (\\mathbf{h} - \\mathbf{y})\n",
    "$$\n",
    "\n",
    "Definimos:\n",
    "\n",
    "- $X$: matriz $m \\times n$ (filas = $x^{(i)T}$)\n",
    "- $\\mathbf{h} = \\sigma(X\\theta)$: vector $m \\times 1$\n",
    "- $\\mathbf{y}$: vector de etiquetas reales $m \\times 1$\n",
    "\n",
    "### Descenso de gradiente\n",
    "\n",
    "Para obtener los valores de $\\theta$ optimos utilizamos la gradiente calculada anteriormente para iterar sobre estos valores $\\theta$ hasta obtener el optimo. Ademas la función de costo es convexa, por lo que el Gradiente Descendente (o cualquier otro algoritmo de optimización) tiene garantizado encontrar el mínimo global.\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\nabla_\\theta J(\\theta)\n",
    "= \\theta - \\alpha \\left( \\frac{1}{m} X^T(\\mathbf{h}-\\mathbf{y}) \\right)\n",
    "$$\n",
    "\n",
    "donde $\\alpha$ es la **tasa de aprendizaje** (*learning rate*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7940dc",
   "metadata": {},
   "source": [
    "### Implementacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "299526aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de0088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(X):\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    return X_b\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def gradientDescent(lr,n_epochs,X, y):\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    theta = np.random.randn(n,1)\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        h = sigmoid(X @ theta)\n",
    "        gradients = 1/m * X.T @ (h - y)\n",
    "        theta -= lr * gradients\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e11caf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parámetros aprendidos (θ):\n",
      "[[ 1.32828484]\n",
      " [ 0.93522895]\n",
      " [-1.99756048]\n",
      " [ 0.51034477]]\n"
     ]
    }
   ],
   "source": [
    "m = 100   # Cantidad de datos\n",
    "n = 3     # Features\n",
    "\n",
    "X = 2 * np.random.randn(m, n)\n",
    "X_b = add_bias(X)\n",
    "\n",
    "true_theta = np.array([[2], [1.5], [-2], [0.5]])\n",
    "\n",
    "p = sigmoid(X_b @ true_theta)\n",
    "y = (np.random.rand(m, 1) < p).astype(int)\n",
    "\n",
    "theta = gradientDescent(0.1, 1000, X_b, y)\n",
    "\n",
    "print(\"\\nParámetros aprendidos (θ):\")\n",
    "print(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86121b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Resultados Sklearn ---\n",
      "Intercepto (Bias): [2.40781834]\n",
      "Coeficientes: [[ 2.06383331 -2.06768501  1.11541491]]\n"
     ]
    }
   ],
   "source": [
    "m = 100   # Cantidad de datos\n",
    "n = 3     # Features\n",
    "\n",
    "X = 2 * np.random.randn(m, n)\n",
    "X_b = add_bias(X)\n",
    "\n",
    "true_theta = np.array([[2], [1.5], [-2], [0.5]])\n",
    "\n",
    "p = sigmoid(X_b @ true_theta)\n",
    "y = (np.random.rand(m, 1) < p).astype(int)\n",
    "\n",
    "model = LogisticRegression(C=1e5)\n",
    "\n",
    "model.fit(X, y.ravel())\n",
    "\n",
    "print(\"--- Resultados Sklearn ---\")\n",
    "print(f\"Intercepto (Bias): {model.intercept_}\")\n",
    "print(f\"Coeficientes: {model.coef_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
