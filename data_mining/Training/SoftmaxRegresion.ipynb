{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e39179d",
   "metadata": {},
   "source": [
    "# Regresión Softmax\n",
    "\n",
    "La regresión Softmax (o regresión logística multinomial) es una generalización de la regresión logística para el caso en el que queremos manejar múltiples clases. En la regresión logística, asumíamos que las etiquetas eran binarias: $y^{(i)} \\in \\{0, 1\\}$. Usábamos dicho clasificador para distinguir entre dos tipos de dígitos escritos a mano. La regresión Softmax nos permite manejar $y^{(i)} \\in \\{1, \\dots, K\\}$, donde $K$ es el número de clases.\n",
    "\n",
    "Es más difícil entrenar el modelo con carios valores de puntuación, ya que es difícil diferenciarlos al implementar el algoritmo de Descenso de Gradiente para minimizar la función de coste. Por lo tanto, necesitamos una función que normalice las puntuaciones logit y las haga fácilmente diferenciables. \n",
    "\n",
    "Asi que primeramente el modelo de Regresión Softmax calcula una puntuación $s_k(x)$ para cada clase $k$. La ecuación para calcular $s_k(x)$ es ;la siguiente:\n",
    "\n",
    "$$\n",
    "s_k(\\mathbf{x}) = \\mathbf{x}^T \\theta^{(k)}\n",
    "$$\n",
    "\n",
    "Luego estima la probabilidad de cada clase aplicando la función softmax (también llamada exponencial normalizada) a dichas puntuaciones, esta calcula la exponencial de cada puntuación, y luego las normaliza como se ve en la siguiente ecuacion:\n",
    "\n",
    "$$\n",
    "\\hat{p}_k = \\sigma(\\mathbf{s}(\\mathbf{x}))_k = \\frac{\\exp(s_k(\\mathbf{x}))}{\\sum_{j=1}^{K} \\exp(s_j(\\mathbf{x}))}\n",
    "$$\n",
    "\n",
    "- $K$ es el número de clases.\n",
    "- $\\mathbf{s}(\\mathbf{x})$ es un vector que contiene las puntuaciones (scores) de cada clase para la instancia $x$.\n",
    "- $\\sigma(\\mathbf{s}(\\mathbf{x}))_k$ es la probabilidad estimada de que la instancia $x$ pertenezca a la clase $k$, dado el puntaje de cada clase.\n",
    "\n",
    "Al igual que el clasificador de Regresión Logística, el clasificador de Regresión Softmax predice la clase con la probabilidad estimada más alta utilizando el operador argmax, que devuelve el valor de la clase que hace que el valor evaluado sea mas alta, se representa como la siguiente ecuacion:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\underset{k}{\\text{argmax}} \\ \\sigma(\\mathbf{s}(\\mathbf{x}))_k = \\underset{k}{\\text{argmax}} \\ s_k(\\mathbf{x}) = \\underset{k}{\\text{argmax}} \\ \\left( \\theta^{(k)T} \\mathbf{x} \\right)\n",
    "$$\n",
    "\n",
    "### Función de costo\n",
    "\n",
    "La función de costo de la Regresión Softmax evalúa qué tan bien (o mal) está funcionando el modelo para clasificar múltiples categorías. Se conoce comúnmente como Entropía Cruzada (Cross-Entropy Loss) o Log Loss Multinomial. Para un conjunto de entrenamiento con $m$ ejemplos y $K$ clases, la función de costo $J(\\Theta)$ se define como:\n",
    "\n",
    "$$\n",
    "J(\\Theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log(\\hat{p}_k^{(i)})\n",
    "$$\n",
    "\n",
    "- $y_k^{(i)}$ es la probabilidad objetivo de que la $i$-ésima instancia pertenezca a la clase $k$. En general, es igual a 1 o 0, dependiendo de si la instancia pertenece a la clase o no\n",
    "\n",
    "### Descenso de gradiente\n",
    "\n",
    "Para realizar el algoritmo del descenso de la gradiente en esta regresion, necesitamos calcular la gradiente de la funcion de costo, planteada anteriormente. Asi que el vector de gradiente de esta función de costo con respecto a $\\theta^{(k)}$ viene dado por la siguiente ecuacion matricial:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta^{(k)}} J(\\mathbf{\\Theta}) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{p}_k^{(i)} - y_k^{(i)} \\right) \\mathbf{x}^{(i)}\n",
    "$$\n",
    "\n",
    "Ya teniendo la gradiente, el descenso de la gradiente estaria dada por la siguiente ecuacion:\n",
    "\n",
    "$$\n",
    "\\theta^{(k)} := \\theta^{(k)} - \\alpha \\nabla_{\\theta^{(k)}} J(\\theta)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dfa0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85883217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(X):\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    return X_b\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True)) # Evita el overflow\n",
    "    return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "\n",
    "def gradientDescent(lr,n_epochs,X, y, K):\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    theta = np.random.randn(n,K)\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        P = softmax(X @ theta)\n",
    "        gradients = 1/m * X.T @ (P - y)\n",
    "        theta -= lr * gradients\n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6939556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parámetros aprendidos (θ):\n",
      "[[ 1.45200379 -0.78303333  4.54764552]\n",
      " [ 2.96389217 -0.10875816 -3.01754123]\n",
      " [-1.51671955  1.24066866  1.96553526]\n",
      " [ 0.11225326 -1.05249668  1.29838315]]\n"
     ]
    }
   ],
   "source": [
    "m = 100   # Cantidad de datos\n",
    "n = 3     # Features\n",
    "K = 3     # Número de clases\n",
    "\n",
    "X = 2 * np.random.randn(m, n)\n",
    "X_b = add_bias(X)\n",
    "\n",
    "true_theta = np.array([\n",
    "    [1.0, -2.0, 2.5],\n",
    "    [2.0,  1.0, -1.5],\n",
    "    [-1.0, 0.5, 1.0],\n",
    "    [0.5, -0.5, 1.0]\n",
    "])\n",
    "\n",
    "scores = X_b @ true_theta\n",
    "y_labels = np.argmax(softmax(scores), axis=1)\n",
    "\n",
    "Y = np.eye(K)[y_labels]\n",
    "\n",
    "theta = gradientDescent(0.1, 2000, X_b, Y, K)\n",
    "\n",
    "print(\"\\nParámetros aprendidos (θ):\")\n",
    "print(theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca6baf",
   "metadata": {},
   "source": [
    "### Implementacion con sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9211acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Resultados Sklearn ---\n",
      "Precisión: 100.00%\n",
      "\n",
      "Intercepto (Bias aprendido):\n",
      "[  6.14135859 -51.27568435  45.13432576]\n",
      "\n",
      "Coeficientes (Pesos aprendidos):\n",
      "[[ 40.59541507  16.6838924  -57.27930747]\n",
      " [-28.6748934    6.2853684   22.389525  ]\n",
      " [  2.65615043 -18.22750125  15.57135082]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "m, n, K = 100, 3, 3\n",
    "X = 2 * np.random.randn(m, n)\n",
    " \n",
    "true_theta = np.array([\n",
    "    [1.0, -2.0, 2.5], [2.0, 1.0, -1.5],\n",
    "    [-1.0, 0.5, 1.0], [0.5, -0.5, 1.0]\n",
    "])\n",
    "\n",
    "X_b = add_bias(X)\n",
    "scores = X_b @ true_theta\n",
    "y = np.argmax(scores, axis=1)\n",
    "\n",
    "softmax_reg = LogisticRegression(solver='lbfgs', C=1e5)\n",
    "\n",
    "softmax_reg.fit(X, y)\n",
    "\n",
    "y_pred = softmax_reg.predict(X)\n",
    "\n",
    "# --- Resultados ---\n",
    "print(\"--- Resultados Sklearn ---\")\n",
    "print(f\"Precisión: {accuracy_score(y, y_pred) * 100:.2f}%\")\n",
    "print(\"\\nIntercepto (Bias aprendido):\")\n",
    "print(softmax_reg.intercept_)\n",
    "print(\"\\nCoeficientes (Pesos aprendidos):\")\n",
    "print(softmax_reg.coef_.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
