{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be27c1f2",
   "metadata": {},
   "source": [
    "# Modelos Lineales Regularizados\n",
    "\n",
    "En muchas técnicas de aprendizaje automático, el aprendizaje consiste en encontrar los coeficientes que minimizan una función de coste. La regularización consiste en añadir una penalización a la función de coste. Esta penalización produce modelos más simples que generalizan mejor.\n",
    "\n",
    "Cuando usamos regularización, añadimos un término que penaliza la complejidad del modelo. En el caso del MSE, tenemos:\n",
    "\n",
    "$$\n",
    "J = MSE + \\lambda \\times C\n",
    "$$\n",
    "\n",
    "- C es la medida de complejidad del modelo. Dependiendo de cómo midamos la complejidad, tendremos distintos tipos de regularización. \n",
    "- λ indica cómo de importante es para nosotros que el modelo sea simple en relación a cómo de importante es su rendimiento.\n",
    "\n",
    "Cuando usamos regularización minimizamos la complejidad del modelo a la vez que minimizamos la función de coste. Esto resulta en modelos más simples que tienden a generalizar mejor.\n",
    "\n",
    "---\n",
    "- Cuando utilices los regulizadores, debes asegurarte de que todas las características tengan una escala similar (por ejemplo, usando la clase `StandardScaler` de **Scikit-Learn**); de lo contrario, el algoritmo tardará mucho más tiempo en converger.\n",
    "\n",
    "    ```python\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    ```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b446dc",
   "metadata": {},
   "source": [
    "## Regularizacion Lasso (L1)\n",
    "\n",
    "La Regresión Lasso es una versión regularizada de la Regresión Lineal: se añade un término de regularización igual a $\\alpha \\sum_{i=1}^{n} |\\theta_i|$ a la función de costo.\n",
    "\n",
    "La funcion de costo de la regresion ridge es:\n",
    "\n",
    "$$\n",
    "J(\\theta) = MSE(\\theta) + \\alpha \\sum_{i=1}^{n} |\\theta_i|\n",
    "$$\n",
    "\n",
    "El hiperparámetro $\\alpha$ controla cuánto deseas regularizar el modelo. Si $\\alpha = 0$, entonces la Regresión Ridge es simplemente una Regresión Lineal. Si $\\alpha$ es muy grande, entonces todos los pesos terminan muy cerca de cero y el resultado es una línea plana que pasa por la media de los datos.\n",
    "\n",
    "La función de costo de Lasso no es diferenciable en $\\theta_i = 0$ (para $i = 1, 2, \\dots, n$), pero el Descenso de Gradiente sigue funcionando bien si utilizas en su lugar un vector de subgradiente $g$ cuando cualquier $\\theta_i = 0$\n",
    "\n",
    "$$\n",
    "g(\\theta, \\lambda) = \\nabla_\\theta \\text{MSE}(\\theta) + \\alpha \n",
    "\\begin{pmatrix}\n",
    "\\text{sign}(\\theta_1) \\\\\n",
    "\\text{sign}(\\theta_2) \\\\\n",
    "\\vdots \\\\\n",
    "\\text{sign}(\\theta_n)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "donde\n",
    "\n",
    "$$\n",
    "\\text{sign}(\\theta_i) =\n",
    "\\begin{cases}\n",
    "-1 & \\text{si } \\theta_i < 0 \\\\\n",
    "0 & \\text{si } \\theta_i = 0 \\\\\n",
    "+1 & \\text{si } \\theta_i > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### Cuándo usar Ridge\n",
    "- Lasso es muy útil cuando sospechamos que varios de los atributos son irrelevantes.\n",
    "- Algunos de los coeficientes acabarán valiendo 0, filtrando así atributos irrelevantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9e4ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes: [2.5159679]\n",
      "Intercepto: [5.04734379]\n",
      "La prediccion es:[8.82129564]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X  + np.random.rand(100, 1)\n",
    "\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "\n",
    "print(\"Coeficientes:\", lasso_reg.coef_)\n",
    "print(\"Intercepto:\", lasso_reg.intercept_)\n",
    "print(f\"La prediccion es:{lasso_reg.predict([[1.5]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66d1d80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes: [3.33167404]\n",
      "Intercepto: [4.09808535]\n",
      "La prediccion es:[9.09559641]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jharv\\OneDrive\\Documentos\\Cursos\\learning-notes\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X  + np.random.rand(100, 1)\n",
    "\n",
    "sgd_reg = SGDRegressor(penalty=\"l1\")\n",
    "sgd_reg.fit(X, y)\n",
    "\n",
    "print(\"Coeficientes:\", sgd_reg.coef_)\n",
    "print(\"Intercepto:\", sgd_reg.intercept_)\n",
    "print(f\"La prediccion es:{sgd_reg.predict([[1.5]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e972376",
   "metadata": {},
   "source": [
    "## Regularizacion Ridge (L2)\n",
    "\n",
    "La Regresión Ridge (también llamada regularización de Tikhonov) es una versión regularizada de la Regresión Lineal: se añade un término de regularización igual a $\\alpha \\sum_{i=1}^{n} \\theta_i^2$ a la función de costo.\n",
    "\n",
    "La funcion de costo de la regresion ridge es:\n",
    "\n",
    "$$\n",
    "J(\\theta) = MSE(\\theta) + \\alpha \\frac{1}{2} \\sum_{i=1}^{n} \\theta_i^2\n",
    "$$\n",
    "\n",
    "El hiperparámetro $\\alpha$ controla cuánto deseas regularizar el modelo. Si $\\alpha = 0$, entonces la Regresión Ridge es simplemente una Regresión Lineal. Si $\\alpha$ es muy grande, entonces todos los pesos terminan muy cerca de cero y el resultado es una línea plana que pasa por la media de los datos.\n",
    "\n",
    "Para la actualizacion de la gradienta, seria sencillo ya que solamente se aumentaria un termino ya que la derivada de $\\frac{1}{2} \\alpha \\mathbf{w}^2$ con respecto a $\\mathbf{w}$ es simplemente $\\alpha \\mathbf{w}$, donde $\\mathbf{w}$ es el vector que contiene todos los parámetros excepto el primero: $\\mathbf{w} = [\\theta_1, \\theta_2, \\dots, \\theta_n]^T$.\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} J(\\theta) = \\text{Gradiente del MSE} + \\alpha \\mathbf{w}\n",
    "$$\n",
    "\n",
    "Al igual que con la Regresión Lineal, podemos realizar la Regresión Ridge ya sea calculando una ecuación de forma cerrada o mediante el Descenso de Gradiente. Los pros y los contras son los mismos. La siguiente ecuacion muestra la solución de forma cerrada (donde $A$ es la matriz identidad de $(n + 1) \\times (n + 1)$, excepto con un 0 en la fila superior y columna izquierda, correspondiente al término de sesgo).\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = (X^T X + \\alpha A)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "#### Cuándo usar Ridge\n",
    "- Cuando dos o más variables explican casi lo mismo (alta colinealidad), el modelo lineal normal tiende a tener coeficientes muy grandes e inestables ($x_2 = 2x_1$).\n",
    "- Cuando el número de características es grande o comparable al número de muestras ($n = m$), el modelo puede sobreajustar fácilmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe05440",
   "metadata": {},
   "source": [
    "### Implementacion con sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf7fca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes: [2.9027427]\n",
      "Intercepto: [4.56236206]\n",
      "La prediccion es:[8.91647611]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X  + np.random.rand(100, 1)\n",
    "\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "\n",
    "print(\"Coeficientes:\", ridge_reg.coef_)\n",
    "print(\"Intercepto:\", ridge_reg.intercept_)\n",
    "print(f\"La prediccion es:{ridge_reg.predict([[1.5]])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adcb049d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes: [3.31672044]\n",
      "Intercepto: [4.00375776]\n",
      "La prediccion es:[8.97883842]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jharv\\OneDrive\\Documentos\\Cursos\\learning-notes\\env\\Lib\\site-packages\\sklearn\\utils\\validation.py:1352: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X  + np.random.rand(100, 1)\n",
    "\n",
    "sgd_reg = SGDRegressor(penalty=\"l2\")\n",
    "sgd_reg.fit(X, y)\n",
    "\n",
    "print(\"Coeficientes:\", sgd_reg.coef_)\n",
    "print(\"Intercepto:\", sgd_reg.intercept_)\n",
    "print(f\"La prediccion es:{sgd_reg.predict([[1.5]])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e85600",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "\n",
    "Elastic Net es un punto medio entre la Regresión Ridge y la Regresión Lasso. El término de regularización es una mezcla simple de los términos de regularización de Ridge y Lasso, y puedes controlar la relación de mezcla $r$. Cuando $r = 0$, Elastic Net es equivalente a la Regresión Ridge, y cuando $r = 1$, es equivalente a la Regresión Lasso\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\text{MSE}(\\theta) + r\\alpha \\sum_{i=1}^{n} |\\theta_i| + \\frac{1 - r}{2} \\alpha \\sum_{i=1}^{n} \\theta_i^2\n",
    "$$\n",
    "\n",
    "Para la actualizacion de la gradiente, seria sumar los terminos de la gradiente de lasso y ridge, como la siguiente formmula:\n",
    "\n",
    "$$\n",
    "g = \\nabla_{\\theta} \\text{MSE}(\\theta) + r\\alpha \\cdot \\text{sign}(\\mathbf{w}) + (1 - r)\\alpha \\mathbf{w}\n",
    "$$\n",
    "\n",
    "Entonces, ¿cuándo deberías usar la Regresión Lineal simple (es decir, sin ninguna regularización), Ridge, Lasso o Elastic Net? Casi siempre es preferible tener al menos un poco de regularización, por lo que, en general, deberías evitar la Regresión Lineal simple.\n",
    "\n",
    "Ridge es un buen valor predeterminado, pero si sospechas que solo unas pocas características son realmente útiles, deberías preferir Lasso o Elastic Net, ya que tienden a reducir los pesos de las características inútiles a cero. En general, Elastic Net se prefiere sobre Lasso, ya que Lasso puede comportarse de forma errática cuando el número de características es mayor que el número de instancias de entrenamiento o cuando varias características están fuertemente correlacionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d39bfe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes: [2.53396632]\n",
      "Intercepto: [4.9576093]\n",
      "La prediccion es:[8.75855878]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X  + np.random.rand(100, 1)\n",
    "\n",
    "elastic_reg = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_reg.fit(X, y)\n",
    "\n",
    "print(\"Coeficientes:\", elastic_reg.coef_)\n",
    "print(\"Intercepto:\", elastic_reg.intercept_)\n",
    "print(f\"La prediccion es:{elastic_reg.predict([[1.5]])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
